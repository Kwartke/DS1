\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....

\usepackage{graphicx} % support the \includegraphics command and options

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{Predicting Prices in German Real Estate Market}
\author{Hanna Martens (7266768) \\ Konrad Wartke (6481209)}
\date{}

\begin{document}
\maketitle

\section{Original Data Sources}

We started with two datasets from Kaggle, one with rent price and one with house prices for the German Real Estate market. The Rent data was scraped from Immoscout24 on three different dates (22.09.2018, 10.05.2019, 08.10.2019). The house prices were also taken from Immoscout24. While no date is given for the creation of this dataset, it is most likely from novembre 2017. The name of the house price dataset is misleading, it features houses from all over Germany instead of Hesse only. \\

\section{Cleanup and Merging}

Since both datasets are compiled from the same original source, they share most of the features. For several features, the names are in German in one dataset, but English the other, so we had to translate the names. \\
The most important feature is the price, which is influenced by most features. For example, all other things equal, having a garden increases the price. To eliminate the influence of several features, we trained a linear regression model. The feature weight is then used to calculate a clean price as if the real estate had no extras. This price can be negative, which means money has to be invested to bring the house into a standard state. \\
At the end, all rent and buying prices where grouped by the zip code and the average rent and buying price per square metre is calculated. Also included are the percentages of real estate units for which a binary feature is true. \\
For future versions, we could also integrate several more of the categorical features, which are currently ignored during merging.

\section{Prediction}
\graphicspath{ {/Users/hanna/PycharmProjects/DS1/simple_linear_regression.png} }
\graphicspath{ {/Users/hanna/PycharmProjects/DS1/multiple_linear_regression.png} }

\subsection{Simple Linear Regression}

The implemented function predicts the target values for any given features, where the column\_y is predicted based on column\_x. To implement the regression model we used Scikit-learn a free software machine learning library for python.
The datasets that were used for the linear regression model are cleaned\_rent and cleaned\_house\_prices.

\begin{figure}[h!]
  \includegraphics[width=14cm, height=10cm]{simple_linear_regression}
  \caption{Number of rooms and total rent}
  \label{fig:sregression}
\end{figure}

\textbf{Evaluation}


The evaluation observed here is exemplary based on the prediction of the rental price based on the number of rooms. As seen in figure \ref{fig:sregression}) the algorithm performs quite poorly. While a few predicted values closely match the actual values, other values are quite off. Exceptions like negative prices for houses which are in need of financial investment or generally missing values may have caused problems here. 
These outliers in the values could haven been detected and dealt with even more carefully. Rows with incorrect data, such as NAN or infinity values, were deleted for this project, which reduced the overall size of the data set. One approach for improvement would be to either replace the faulty data e.g. with average values or to integrate more data to extend the data set.
A series of experiments with possible combination pairs could be another approach to find two features that have a stronger connection to improve the result.

\subsection{Multiple Linear Regression}

Another way to improve the results from the last section is multiple linear regression. We have again set the rental price as prediciton target and this time we include several different features from the data set to train the model.

\begin{figure}[h!]
\centering
  \includegraphics[width=14cm, height=10cm]{multiple_linear_regression.png}
  \caption{Multiple features and total rent}
  \label{fig:mregression}
\end{figure}

\textbf{Evaluation}

The quality of the algorithms can also be assessed directly by the Scikit-learn metrics library.\\

{
\centering
\begin{tabular}{ |c|c|c| } 
\hline 
Metric & simple regression & multiple regression \\
 \hline
 Mean Absolute Error & 389.5304 & 195.7756 \\
 \hline
 Mean Squared Error & 219942.1611 & 50782.9660 \\
 \hline
 Root Mean Squared Error & 468.9799 &225.3507 \\ 
 \hline
\end{tabular}\\
}\hfill

Mean squared error (MSE) - takes the mean squared difference between the target and predicted values
Mean absolute error (MAE) - calculates prediction errors (actual value minus predicted value) for each row of data and find the mean of all absolute prediction errors.
Root Mean Squared Error (RMSD) - is the standard deviation of the prediction errors.\\

{
\centering
\begin{tabular}{ |c|c|c| } 
 \hline
 & Coefficient \\ 
 \hline
 Wohnflaeche in mÂ² & 6.376861 \\ 
Zimmer & 65.611050 \\ 
baseRentRange & 147.183205 \\
livingSpaceRange & -35.609006 \\
yearConstructed & -2.467776 \\
numberOfFloors & 28.994673. \\
 \hline
\end{tabular} \\
}\hfill

The regression model finds the best fitting coefficients for all attributes and weights them accordingly.
Here we see that, for example, the year of construction has no influence on the rent, while the rooms, number of square meters and baseRentRage are decisive.\\
Overall it seems like including several features instead of just looking at two features at a time improved the performance of our model.

\subsection{KNN}

The k nearest neighbor (KNN) algorithm is a type of supervised machine learning and is used for both classification and regression problems. 
We used the algorithm to predict the average rent per square meter based on all other values contained in the merged dataset.
For our regression problem we used the Scikit-learn knn model to implement the algorithm for our dataset. Since there is no optimal number of neighbors that suits all kind of data sets fitting the model was mostly trial and error. For our dataset the optimal number of neighbors is five, while the train set contains 90\% of the data and the test set accordingly contains 10\%. \\

{
\centering
\begin{tabular}{ |c|c|c| } 
\hline 
Metric & KNN Model \\
 \hline
 Mean Absolute Error & 3.160\\
 \hline
 Mean Squared Error & 89.093 \\
 \hline
 Root Mean Squared Error & 9.438 \\ 
 \hline
\end{tabular}\\
}

\section{Results}

If we compare the algorithms with each other we can see a steady improvement of the values. While multiple linear regression is much better than single linear regression, the knn algorithm finds the best prediction.
Obviously the linear regression models and the knn model used different sets of data thus are not fully comparable.
With more time, the knn model could be trained with the data used in linear regression and a final statement could be made about which algorithm makes the best prediction for our data set.
 \\
The kNN-model of using the nearest examples in feature space agree with common sense. An real life approach to predicting real estate prices is using similiar units in the same area. \\
For further attempts, the assumption of linear and indepedent features could be loosened. Also engineered features such as average income in a region, distance to public transport or categorizing area via zip code into urban, suburban or rural could further imporve results.

\section{References}

Dataset 1: \\
https://www.kaggle.com/phanindraparashar/germany-housing-rent-and-price-data-set-apr-20 \\
Dataset 2: \\
https://www.kaggle.com/orgesleka/hessen-house-prices-dataset \\
Coderepo: \\
https://github.com/Kwartke/DS1

\end{document}
